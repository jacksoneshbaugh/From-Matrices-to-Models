{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# XOR ANN implemented in Pure Python\n",
    "This ANN is based on the discussion in section 4 of _\"From Matrices to Models.\"_ In this notebook, you'll find the code needed to manually implement the ANN that solves XOR, along with annotations and comments that reference parts of the paper.\n",
    "\n"
   ],
   "id": "8365b182a8e68a9e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Firstly, we setup our imports, and we define the two activation functions we'll use later, which are ReLU (Equation 3.4) and sigmoid (Equation 3.5). Notice that we define both to operate on _each entry in a vector or matrix_—this is key.",
   "id": "82c50cd0cdc03bec"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-26T21:12:58.545700Z",
     "start_time": "2024-11-26T21:12:58.456076Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from numpy import floating\n",
    "\n",
    "# Equation (3.4)\n",
    "def relu(z) -> np.ndarray:\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "# The derivative of Equation (3.4)\n",
    "def relu_derivative(z: np.ndarray) -> np.ndarray:\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "# Equation (3.5)\n",
    "def sigmoid(z: np.ndarray) -> np.ndarray:\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# The derivative of Equation (3.5)\n",
    "def sigmoid_derivative(z: np.ndarray) -> np.ndarray:\n",
    "    return sigmoid(z) * (1 - sigmoid(z))"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Next, we define our:\n",
    "- input data (`X`), a matrix;\n",
    "- expected outputs (`y`), a vector;\n",
    "- and our hyperparameters.\n",
    "\n",
    "The hyperparameters describe the structure of the network (how many neurons in each layer) and learning algorithm parameters (the learning rate and number of epochs to train for). Specifically, `learning_rate` is $\\eta$ in Equation 4.3, and `training_epochs` represents how many times we should run the training algorithm. These numbers both must be tuned on a network-by-network basis—trial and error is almost guaranteed. Specifically, training too much can lead to an overfit algorithm (and too little leads to an underfit algorithm) [9]."
   ],
   "id": "8a5c6a23d4965588"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T21:12:58.554625Z",
     "start_time": "2024-11-26T21:12:58.548610Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Setup input/output data for xor\n",
    "X: np.ndarray = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y: np.ndarray = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Hyperparameters\n",
    "input_size: int = 2             # 2 neurons in the input layer\n",
    "hidden_layer_size: int = 2      # 2 neurons in the hidden layer\n",
    "output_size: int = 1            # 1 neuron in the output layer\n",
    "\n",
    "learning_rate: float = 0.1     # eta in equation 4.3\n",
    "\n",
    "training_epochs: int = 10000    # how many times to run the training algorithm (fine tune \n",
    "                                # this number, since the network can over- or under-fit if\n",
    "                                # this number is too large or small—see [9] for more info)"
   ],
   "id": "47dac35c3e073672",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now, we initialize the weights and biases for the network to random values. `np.random.randn(rows, columns)` generates a random `ndarray` (matrix or vector) of numbers. Then, as described in Equation (3.3), transitions between layers can be thought of as linear transformations, such that:\n",
    "-  $T_1 : \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2$ (input layer $\\rightarrow$ hidden layer) has weight matrix $W_1$ and bias vector $b_1$\n",
    "- $T_2 : \\mathbb{R}^2 \\rightarrow \\mathbb{R}$ (hidden layer $\\rightarrow$ output layer) has weight matrix $W_2$ and bias vector $b_2$\n",
    "\n",
    "We scale the random numbers—in matrices $W_1$ and $W_2$, we use what is called Xavier initialization. This scales the weights so that the variance of the activations is the same across layers, helping to ensure that the network converges to a global minimum during training [10]."
   ],
   "id": "ee05840a1ad5ed83"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T21:12:59.802499Z",
     "start_time": "2024-11-26T21:12:58.556774Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize weights and biases\n",
    "\n",
    "np.random.seed(38)  # for reproducibility\n",
    "\n",
    "# T_1 : input layer -> hidden layer (R^2 -> R^2)\n",
    "W1: np.ndarray = np.random.randn(input_size, hidden_layer_size) * np.sqrt(2. / input_size) # Scale the weights\n",
    "                                                                    # so that the variance of activations is\n",
    "                                                                    # the same across layers (Xavier Initialization)\n",
    "b1: np.ndarray = np.random.randn(1, hidden_layer_size) * 0.1\n",
    "\n",
    "# T_2 : hidden layer -> output layer (R^2 -> R)\n",
    "W2: np.ndarray = np.random.randn(hidden_layer_size, output_size) * np.sqrt(2. / input_size)\n",
    "b2: np.ndarray = np.random.randn(1, output_size) * 0.1"
   ],
   "id": "f7a4ca148726d020",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Next, it's time to train the network. This is done in a two-step process:\n",
    "1. We do a forward pass, computing the current network estimation. We calculate the cost function—in the case of this network, MSE: $J(\\textbf{w}, b) = \\frac{1}{4}\\sum_{\\textbf{d} \\in D} (f^*(\\textbf{d}) - f(\\textbf{d}, \\textbf{w}, b))$ (Equation 4.1)\n",
    "        \n",
    "2. We perform batch gradient descent and backpropagation, described by Equation 4.3:\n",
    "$\\textbf{w} = \\textbf{w} - \\eta \\cdot \\nabla J(\\textbf{w})$"
   ],
   "id": "4fce88d299f4d8e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T21:13:00.200585Z",
     "start_time": "2024-11-26T21:12:59.803764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train the network\n",
    "for epoch in range(training_epochs):\n",
    "    # Forward pass\n",
    "    z1: np.ndarray = np.dot(X, W1) + b1 # Evaluate the inside of the ReLU function\n",
    "    a1: np.ndarray = relu(z1) # Apply ReLU\n",
    "    \n",
    "    z2: np.ndarray = np.dot(a1, W2) + b2\n",
    "    a2: np.ndarray = sigmoid(z2)\n",
    "    \n",
    "    error: np.ndarray = y - a2\n",
    "    cost: floating = np.mean(error**2)\n",
    "    \n",
    "    # Backpropagation (Equation 4.3)\n",
    "    d_a2: np.ndarray = error * sigmoid_derivative(a2)            # Estimate gradient at output\n",
    "    d_W2: np.ndarray = np.dot(a1.T, d_a2)                        # Get the weight update for W2\n",
    "    d_b2: np.ndarray = np.sum(d_a2, axis=0, keepdims=True)       # Get the bias update for b2\n",
    "    \n",
    "    d_a1: np.ndarray = np.dot(d_a2, W2.T) * relu_derivative(a1)  # Estimate gradient at hidden layer\n",
    "    d_W1: np.ndarray = np.dot(X.T, d_a1)                         # Get the weight update for W1\n",
    "    d_b1: np.ndarray = np.sum(d_a1, axis=0, keepdims=True)       # Get the bias update for b1\n",
    "    \n",
    "    # Update the weights and biases\n",
    "    W1 += learning_rate * d_W1\n",
    "    b1 += learning_rate * d_b1\n",
    "    W2 += learning_rate * d_W2\n",
    "    b2 += learning_rate * d_b2\n",
    "    \n",
    "    # Print the cost occasionally\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f'Epoch: {epoch}, Cost: {cost}')"
   ],
   "id": "7053b5eb84a64642",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cost: 0.2837071413735631\n",
      "Epoch: 1000, Cost: 0.11898073301403354\n",
      "Epoch: 2000, Cost: 0.0009480482553462136\n",
      "Epoch: 3000, Cost: 0.00020454082651267882\n",
      "Epoch: 4000, Cost: 8.378823534111242e-05\n",
      "Epoch: 5000, Cost: 4.4712507442895376e-05\n",
      "Epoch: 6000, Cost: 2.748404894141129e-05\n",
      "Epoch: 7000, Cost: 1.8532466873386784e-05\n",
      "Epoch: 8000, Cost: 1.329569505916439e-05\n",
      "Epoch: 9000, Cost: 9.983495112767322e-06\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Our network is trained and good to go! Now, we test it with all the values we trained it on (Equations 4.12 and 4.13).",
   "id": "1aedd74153f8b1b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T21:13:00.208110Z",
     "start_time": "2024-11-26T21:13:00.203651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Testing\n",
    "print(\"Predictions after training:\")\n",
    "for i in range(len(X)):\n",
    "    z1: np.ndarray = np.dot(X[i], W1) + b1\n",
    "    a1: np.ndarray = relu(z1)\n",
    "    z2: np.ndarray = np.dot(a1, W2) + b2\n",
    "    a2: np.ndarray = sigmoid(z2)\n",
    "    print(f\"Input: {X[i]} -> Predicted Output: {a2.round()} (Raw: {a2})\")"
   ],
   "id": "b8abb0b05e15af73",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions after training:\n",
      "Input: [0 0] -> Predicted Output: [[0.]] (Raw: [[0.0006837]])\n",
      "Input: [0 1] -> Predicted Output: [[1.]] (Raw: [[0.99612045]])\n",
      "Input: [1 0] -> Predicted Output: [[1.]] (Raw: [[0.99612045]])\n",
      "Input: [1 1] -> Predicted Output: [[0.]] (Raw: [[0.00068277]])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T21:13:00.215608Z",
     "start_time": "2024-11-26T21:13:00.211933Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "63d95bc585d687ff",
   "outputs": [],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
